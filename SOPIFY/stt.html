<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Interview Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            background: #1a1a1a;
            color: #fff;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        
        .header {
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #333;
        }
        
        .header h1 {
            font-size: 24px;
            font-weight: 600;
        }
        
        .setup {
            padding: 20px;
            border-bottom: 1px solid #333;
        }
        
        .setup input {
            width: 100%;
            padding: 12px;
            border: 1px solid #444;
            border-radius: 8px;
            background: #2a2a2a;
            color: #fff;
            font-size: 14px;
        }
        
        .setup input::placeholder {
            color: #999;
        }
        
        .cv-section {
            padding: 20px;
            border-bottom: 1px solid #333;
        }
        
        .cv-section textarea {
            width: 100%;
            min-height: 120px;
            padding: 12px;
            border: 1px solid #444;
            border-radius: 8px;
            background: #2a2a2a;
            color: #fff;
            font-size: 14px;
            resize: vertical;
            font-family: inherit;
        }
        
        .cv-section textarea::placeholder {
            color: #999;
        }
        
        .controls {
            padding: 20px;
            text-align: center;
            border-bottom: 1px solid #333;
        }
        
        .record-btn {
            padding: 15px 30px;
            border: none;
            border-radius: 50px;
            background: #ef4444;
            color: white;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s;
            min-width: 140px;
        }
        
        .record-btn:hover {
            transform: scale(1.05);
        }
        
        .record-btn.recording {
            background: #22c55e;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        
        .record-btn:disabled {
            background: #666;
            cursor: not-allowed;
            transform: none;
        }
        
        .conversation {
            flex: 1;
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
            width: 100%;
        }
        
        .message {
            margin-bottom: 20px;
            padding: 16px;
            border-radius: 12px;
            max-width: 90%;
            word-wrap: break-word;
        }
        
        .user-message {
            background: #2563eb;
            margin-left: auto;
            text-align: right;
        }
        
        .ai-message {
            background: #374151;
            margin-right: auto;
        }
        
        .status {
            text-align: center;
            color: #999;
            font-size: 14px;
            margin: 10px 0;
        }
        
        .typing {
            display: inline-block;
        }
        
        .typing::after {
            content: '...';
            animation: typing 1.5s infinite;
        }
        
        @keyframes typing {
            0%, 20% { content: '...'; }
            40% { content: '..'; }
            60% { content: '.'; }
            80%, 100% { content: ''; }
        }
        
        .error {
            color: #ef4444;
            text-align: center;
            padding: 10px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Voice Interview Assistant</h1>
    </div>
    
    <div class="setup">
        <input type="password" id="apiKey" placeholder="Enter your OpenAI API key" />
    </div>
    
    <div class="cv-section">
        <textarea id="cvContent" placeholder="Paste your CV content here...

Education:
- [Your education details]

Work Experience:
- [Your work experience]

Projects:
- [Your key projects]

Skills:
- [Your technical skills]

etc..."></textarea>
    </div>
    
    <div class="controls">
        <div style="margin-bottom: 15px;">
            <select id="audioSource" style="padding: 8px; border-radius: 6px; background: #2a2a2a; color: #fff; border: 1px solid #444; margin-right: 10px;">
                <option value="microphone">üé§ Microphone Only</option>
                <option value="screen">üñ•Ô∏è Screen + System Audio</option>
            </select>
            <button id="testAudio" style="padding: 8px 15px; border-radius: 6px; background: #374151; color: #fff; border: 1px solid #444; cursor: pointer;">Test Audio</button>
        </div>
        <button class="record-btn" id="recordBtn" disabled>
            üé§ Start Recording
        </button>
        <div class="status" id="status">Enter API key and CV to begin</div>
    </div>
    
    <div class="conversation" id="conversation">
        <!-- Messages will appear here -->
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let apiKey = '';
        let cvContent = '';
        
        const recordBtn = document.getElementById('recordBtn');
        const status = document.getElementById('status');
        const conversation = document.getElementById('conversation');
        const apiKeyInput = document.getElementById('apiKey');
        const cvInput = document.getElementById('cvContent');
        const audioSourceSelect = document.getElementById('audioSource');
        const testAudioBtn = document.getElementById('testAudio');
        
        // Enable recording when both API key and CV are provided
        function checkSetup() {
            apiKey = apiKeyInput.value.trim();
            cvContent = cvInput.value.trim();
            
            if (apiKey && cvContent) {
                recordBtn.disabled = false;
                status.textContent = 'Ready to record';
            } else {
                recordBtn.disabled = true;
                status.textContent = 'Enter API key and CV to begin';
            }
        }
        
        apiKeyInput.addEventListener('input', checkSetup);
        cvInput.addEventListener('input', checkSetup);
        testAudioBtn.addEventListener('click', testAudio);
        
        // Add message to conversation
        function addMessage(text, isUser = false) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${isUser ? 'user-message' : 'ai-message'}`;
            messageDiv.textContent = text;
            conversation.appendChild(messageDiv);
            conversation.scrollTop = conversation.scrollHeight;
        }
        
        // Show typing indicator
        function showTyping() {
            const typingDiv = document.createElement('div');
            typingDiv.className = 'message ai-message typing';
            typingDiv.id = 'typing-indicator';
            typingDiv.textContent = 'Assistant is thinking';
            conversation.appendChild(typingDiv);
            conversation.scrollTop = conversation.scrollHeight;
        }
        
        // Remove typing indicator
        function removeTyping() {
            const typing = document.getElementById('typing-indicator');
            if (typing) typing.remove();
        }
        
        // Show error
        function showError(message) {
            const errorDiv = document.createElement('div');
            errorDiv.className = 'error';
            errorDiv.textContent = message;
            conversation.appendChild(errorDiv);
            conversation.scrollTop = conversation.scrollHeight;
        }
        
        // Process transcribed text with OpenAI
        async function processWithAI(transcribedText) {
            try {
                showTyping();
                
                const response = await fetch('https://api.openai.com/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        model: 'gpt-4o-mini',
                        messages: [
                            {
                                role: 'system',
                                content: `You are an intelligent virtual interview assistant. 

Context: The user will speak questions in a vague, informal, or paraphrased way. You must understand the intended question behind their words and answer it using the reference information provided in their CV.

Goals:
1. Infer the most likely interview question they're referring to
2. Respond to that question in a crisp, professional tone (as if they were speaking)
3. Only rely on the information available in their CV

Constraints:
- Do not answer meta-questions (e.g., "Did you ask about...")
- If a question is unclear, make the best guess from common interview questions
- Keep answers concise (~100 words)

CV Content:
${cvContent}`
                            },
                            {
                                role: 'user',
                                content: transcribedText
                            }
                        ],
                        max_tokens: 300,
                        temperature: 0.7
                    })
                });
                
                removeTyping();
                
                if (!response.ok) {
                    throw new Error(`API Error: ${response.status}`);
                }
                
                const data = await response.json();
                const aiResponse = data.choices[0].message.content;
                
                addMessage(aiResponse, false);
                
            } catch (error) {
                removeTyping();
                showError(`Error: ${error.message}`);
            }
        }
        
        // Transcribe audio using OpenAI
        async function transcribeAudio(audioBlob) {
            try {
                status.textContent = 'Transcribing...';
                
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.webm');
                formData.append('model', 'gpt-4o-mini-transcribe');
                formData.append('response_format', 'json');
                
                const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: formData
                });
                
                if (!response.ok) {
                    const errorText = await response.text();
                    console.error('API Error:', errorText);
                    throw new Error(`Transcription failed: ${response.status} - ${errorText}`);
                }
                
                const data = await response.json();
                const transcribedText = data.text;
                
                if (transcribedText && transcribedText.trim()) {
                    addMessage(transcribedText, true);
                    await processWithAI(transcribedText);
                } else {
                    showError('No speech detected. Please try again.');
                }
                
                status.textContent = 'Ready to record';
                
            } catch (error) {
                showError(`Transcription error: ${error.message}`);
                status.textContent = 'Ready to record';
            }
        }
        
        // Get audio stream based on selected source
        async function getAudioStream() {
            const audioSource = audioSourceSelect.value;
            
            if (audioSource === 'microphone') {
                return await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 44100
                    } 
                });
            } else if (audioSource === 'screen') {
                try {
                    // Request screen share with audio
                    const screenStream = await navigator.mediaDevices.getDisplayMedia({
                        video: false,
                        audio: {
                            echoCancellation: false,
                            noiseSuppression: false,
                            autoGainControl: false,
                            sampleRate: 44100
                        }
                    });
                    
                    return screenStream;
                } catch (error) {
                    // Fallback to microphone if screen sharing fails
                    showError('Screen audio not available. Using microphone instead.');
                    return await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 44100
                        } 
                    });
                }
            }
        }
        
        // Test audio source
        async function testAudio() {
            try {
                testAudioBtn.disabled = true;
                testAudioBtn.textContent = 'Testing...';
                
                const stream = await getAudioStream();
                
                // Create audio context to analyze the stream
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                const analyser = audioContext.createAnalyser();
                
                source.connect(analyser);
                analyser.fftSize = 256;
                
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                
                let maxVolume = 0;
                let testDuration = 3000; // 3 seconds
                let startTime = Date.now();
                
                function checkAudio() {
                    analyser.getByteFrequencyData(dataArray);
                    const volume = dataArray.reduce((a, b) => a + b) / bufferLength;
                    maxVolume = Math.max(maxVolume, volume);
                    
                    if (Date.now() - startTime < testDuration) {
                        requestAnimationFrame(checkAudio);
                    } else {
                        // Test complete
                        stream.getTracks().forEach(track => track.stop());
                        audioContext.close();
                        
                        if (maxVolume > 5) {
                            status.textContent = `‚úÖ Audio detected! Max volume: ${Math.round(maxVolume)}`;
                        } else {
                            status.textContent = `‚ö†Ô∏è Low/no audio detected. Max volume: ${Math.round(maxVolume)}`;
                        }
                        
                        testAudioBtn.disabled = false;
                        testAudioBtn.textContent = 'Test Audio';
                    }
                }
                
                checkAudio();
                
            } catch (error) {
                showError(`Audio test failed: ${error.message}`);
                testAudioBtn.disabled = false;
                testAudioBtn.textContent = 'Test Audio';
            }
        }
        
        // Start recording
        async function startRecording() {
            try {
                const stream = await getAudioStream();
                
                // Check for supported MIME types
                let mimeType = 'audio/webm;codecs=opus';
                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    mimeType = 'audio/webm';
                    if (!MediaRecorder.isTypeSupported(mimeType)) {
                        mimeType = 'audio/mp4';
                        if (!MediaRecorder.isTypeSupported(mimeType)) {
                            mimeType = ''; // Let browser choose
                        }
                    }
                }
                
                const options = mimeType ? { mimeType } : {};
                mediaRecorder = new MediaRecorder(stream, options);
                
                audioChunks = [];
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };
                
                mediaRecorder.onstop = async () => {
                    if (audioChunks.length > 0) {
                        const audioBlob = new Blob(audioChunks, { 
                            type: mimeType || 'audio/webm' 
                        });
                        
                        // Check if blob has content
                        if (audioBlob.size > 0) {
                            await transcribeAudio(audioBlob);
                        } else {
                            showError('No audio recorded. Please try again.');
                            status.textContent = 'Ready to record';
                        }
                    }
                    
                    // Stop all tracks
                    stream.getTracks().forEach(track => track.stop());
                };
                
                mediaRecorder.start(1000); // Collect data every second
                isRecording = true;
                
                recordBtn.textContent = 'üõë Stop Recording';
                recordBtn.classList.add('recording');
                
                const sourceText = audioSourceSelect.value === 'screen' ? 'screen audio' : 'microphone';
                status.textContent = `Recording ${sourceText}... Click stop when done`;
                
            } catch (error) {
                showError(`Recording error: ${error.message}`);
                status.textContent = 'Ready to record';
            }
        }
        
        // Stop recording
        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                
                recordBtn.textContent = 'üé§ Start Recording';
                recordBtn.classList.remove('recording');
                status.textContent = 'Processing...';
            }
        }
        
        // Record button click handler
        recordBtn.addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });
        
        // Initial setup check
        checkSetup();
    </script>
</body>
</html>